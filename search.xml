<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>redis学习笔记(一)</title>
    <url>/posts/ccb0c92d.html</url>
    <content><![CDATA[<h2 id="1-NoSQL-概念"><a href="#1-NoSQL-概念" class="headerlink" title="1. NoSQL 概念"></a>1. NoSQL 概念</h2><h3 id="1-NoSQL-数据模型简介"><a href="#1-NoSQL-数据模型简介" class="headerlink" title="1. NoSQL 数据模型简介"></a>1. NoSQL 数据模型简介</h3><ul>
<li><p>聚合模型</p>
<ol>
<li><p>KV键值对</p>
</li>
<li><p>Bson: JSON串的数据表达</p>
</li>
<li><p>列族:</p>
</li>
<li><p>图形</p>
<blockquote>
<p><a href="https://github.com/xingbofeng/Reading-Note/blob/master/%E3%80%8ANoSQL%E7%B2%BE%E7%B2%B9%E3%80%8B%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B.md" target="_blank" rel="noopener">聚合模型</a></p>
</blockquote>
</li>
</ol>
</li>
</ul>
<h3 id="2-NoSQL数据库四大分类"><a href="#2-NoSQL数据库四大分类" class="headerlink" title="2. NoSQL数据库四大分类"></a>2. NoSQL数据库四大分类</h3><ol>
<li>KV键值对: redis, memcache, tair, BerkeleyDB</li>
<li>文档型数据库(Bson格式):CouchDB, MongoDB: 基于分布式文件存储的数据库, 介于关系型数据库和非关系数据库之间</li>
<li>列族: HBase, Cassandra, 分布式文件系统</li>
<li>图形: 放的是图谱, Neo4J, InfoGrid</li>
</ol>
<a id="more"></a>

<h3 id="3-分布式数据库中的CAP原理"><a href="#3-分布式数据库中的CAP原理" class="headerlink" title="3. 分布式数据库中的CAP原理"></a>3. 分布式数据库中的CAP原理</h3><ul>
<li><p>传统的关系型数据库是 ACID  </p>
<ul>
<li>Atomicity 原子性  要么都执行，要么都回滚</li>
<li>Consistency 一致性  保证数据的状态操作前和操作后保持一致</li>
<li>Isolation 独立性  多个事务同时操作相同数据库的同一个数据时，一个事务的执行不受另外一个事务的干扰</li>
<li>Durability 持久性  一个事务一旦提交，则数据将持久化到本地，除非其他事务对其进行修改</li>
</ul>
</li>
<li><p>NoSQL 中的 CAP+BASE</p>
<p><img src="./img/cap.jpg" alt="cap"></p>
<ul>
<li>Consitency 强一致性 : 写操作之后的读操作，必须返回该值</li>
<li>Availability 可用性 : 只要收到用户的请求，服务器就必须给出回应</li>
<li>Partition tolerance 分区容错性 : 区间通信可能失败</li>
<li>一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。CAP 定理告诉我们，剩下的 C 和 A 无法同时做到</li>
<li><a href="https://www.ruanyifeng.com/blog/2018/07/cap.html" target="_blank" rel="noopener">CAP定理含义</a></li>
</ul>
</li>
<li><p>CAP的3进2</p>
<ul>
<li>CAP理论的核心是: 一个分布式系统不可以同时很好的满足一致性, 可用性和分区容错性这3个需求.</li>
<li>最多只能同时较好的满足其中2个</li>
<li>CA 单点集群, 满足一致性和可用性, 扩展性不强, 传统的数据库, MySQL, Oracle</li>
<li>CP 满足一致性, 分区容错性, 性能不高, NoSQL, MongoDB,redis,HBase</li>
<li>AP 满足可用性, 分区容错性, 对一致性要求低, CouchDB…,大多数网站架构的选择</li>
</ul>
</li>
<li><p>BASE 是为了解决关系数据库强一致性引起的可用性降低而提出的解决方案</p>
<ul>
<li>Basically Availabel 基本可用</li>
<li>Soft state 软状态</li>
<li>Eventually consistent 最终一致</li>
<li>通过让系统对某一时刻的数据一致性的要求来换取系统整体伸缩性和性能上改观, 即牺牲C换取AP,通过BASE再达到C</li>
</ul>
</li>
</ul>
<h3 id="4-分布式-集群简介"><a href="#4-分布式-集群简介" class="headerlink" title="4. 分布式+集群简介"></a>4. 分布式+集群简介</h3><ul>
<li>分布式: 不同的多台服务器上部署不同的服务模块(工程), 他们之间通过Rpc/Rmi之间通信和调用, 对外提供服务和组内协助</li>
<li>集群: 不同的多台服务器上面部署相同的服务模块, 通过分布式调度软件进行同意的调度, 对外提供服务和访问</li>
</ul>
<h2 id="2-Redis-介绍"><a href="#2-Redis-介绍" class="headerlink" title="2. Redis 介绍"></a>2. Redis 介绍</h2><ul>
<li>REmote DIctionary Server(远程字典服务器)<ul>
<li>C语言写的, 遵守BSD协议, 是一个高性能的KV分布式内存数据库, 基于内存运行, 并支持持久化的NoSQL数据库</li>
<li>支持异步将内存中的数据写到硬盘上, 同时不影响继续服务</li>
<li>取最新N个数据</li>
<li>模拟类似HttpSession这种要设定过期时间的功能</li>
<li>发布,订阅消息系统</li>
<li>定时器, 计数器  </li>
</ul>
</li>
<li>特点:<ul>
<li>redis支持数据的持久化, 可以将内存中的数据保存再磁盘中, 重启的时候可以再次加载使用</li>
<li>redis不仅支持KV存储, 还提供了list,set,zset,hash等数据结构的存储</li>
<li>redis支持数据备份, 即master-slave模式的数据备份</li>
<li>KV + cache + persistent</li>
</ul>
</li>
</ul>
<h3 id="1-Redis的5大数据类型"><a href="#1-Redis的5大数据类型" class="headerlink" title="1. Redis的5大数据类型"></a>1. Redis的5大数据类型</h3><ul>
<li><p>String字符串</p>
<ul>
<li>一个key对应一个value</li>
<li>string 类型是二进制安全的, 可以包含任何数据, 比如图片或序列化的对象</li>
<li>string 是redis最基本的类型, 一个redis中字符串value最多可以是512M</li>
</ul>
</li>
<li><p>Hash哈希</p>
<ul>
<li>是一个键值对集合, 类似Java的Map</li>
<li>redis hash 是一个 string 类的field 和value 的映射表, 适合存储对象, 类似Object&lt;String, Object&gt;</li>
</ul>
</li>
<li><p>List列表</p>
<ul>
<li>简单的字符串列表, 按照插入的顺序排序, 可以添加一个元素到列表的头部或尾部</li>
<li>底层实际是个链表</li>
</ul>
</li>
<li><p>Set集合</p>
<ul>
<li>string类型的无序,不能重复集合, 通过hashTable实现</li>
</ul>
</li>
<li><p>Zset有序集合</p>
<ul>
<li>和set一样是string类型的集合, 不同的是每个元素关联一个double类型的分数, redis通过分数对集合中的成员进行从大到小的排序, zset的成员是唯一的, 分数却可以重复</li>
</ul>
</li>
<li><p><a href="http://redisdoc.com" target="_blank" rel="noopener">redis命令手册</a></p>
</li>
</ul>
<h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><ol>
<li>redis键(key)<ul>
<li>keys *</li>
<li>exists key, 判断key是否存在, 1存在, 0不存在</li>
<li>move key num, 移动key至num号库, 当前库没有该key</li>
<li>expire key 秒数, 给指定key设置过期时间</li>
<li>ttl key 看出key还有多少秒过期, -1永不过期, -2 已过期(自动删除了)</li>
<li>type key 查看key是什么类型</li>
<li>del key 删除该key</li>
<li>set key value 为key 设定value, 若key已存在, 覆盖旧的值</li>
</ul>
</li>
<li>redis字符串(String)<ul>
<li>set/get/del/append/strlen</li>
<li>incr/decr/incrby/decrby 是数字才可以加减</li>
<li>getrange/setrange key 0 3; 获取[0,3]</li>
<li>setex key 12 value;  key 只存活12s</li>
<li>setnx(if not exist) key value;</li>
<li>mset/mget/msetnx</li>
</ul>
</li>
<li>redis列表(List)<ul>
<li>lpush/rpush/lrange</li>
<li>lpop/rpop</li>
<li>lindex</li>
<li>llen</li>
<li>lrem key N value; 删除N个值</li>
<li>ltrim key begin edn; 截取指定范围的值在赋值给key</li>
<li>rpoplpush 源列表 目标列表</li>
<li>lset key index value</li>
<li>linsert key before/after 值1 值2</li>
</ul>
</li>
<li>redis集合(set)<ul>
<li>sadd/smember/sismember</li>
<li>scard 获取集合元素个数</li>
<li>srem key value</li>
<li>srandmember key num;  随机num个出栈</li>
<li>spop key 随机出栈</li>
<li>smove key1 key2 value1;  将key1中的value1赋给key2</li>
<li>数学集合类:<ul>
<li>sdiff 差集</li>
<li>sinter 交集</li>
<li>sunion 并集</li>
</ul>
</li>
</ul>
</li>
<li>redis哈希(Hash)<ul>
<li>KV模式不变, 但V是一个键值对</li>
<li>hset/hget/hmset/hmget/hgetall/hdel</li>
<li>hlen</li>
<li>hexists key 在key中存在的某个值的key</li>
<li>hkeys / hvals</li>
<li>hincrby/hincrbyfloat</li>
<li>hsetnx</li>
</ul>
</li>
<li>redis有序集合Zset(sorted set)<ul>
<li>在set的基础上,加一个score值, set是 k1 v1 v2 v3; zset 是 k1 score1 v1 score2 v2</li>
<li>zadd / zrange</li>
<li>zrangebyscore key 分数范围  ‘(‘ 表示不包含,  eg: 60 90 -&gt; [60,90] (60 (90 -&gt; (60,90)</li>
<li>zrem key 某score下对应的value值, 删除元素</li>
<li>zcard/zcount key score区间 / zrank key values值 作用是获得下标值/zscore key 对应值, 获得分数</li>
<li>zrevrand key values, 逆序获得下标值</li>
<li>zrevrange</li>
<li>zrevrangebyscore key</li>
</ul>
</li>
</ol>
<h3 id="2-解析配置文件-redis-conf"><a href="#2-解析配置文件-redis-conf" class="headerlink" title="2. 解析配置文件 redis.conf"></a>2. 解析配置文件 redis.conf</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Note on units: when memory size is needed, it is possible to specify</span><br><span class="line"># it in the usual form of 1k 5GB 4M and so forth:</span><br><span class="line">#</span><br><span class="line"># 1k &#x3D;&gt; 1000 bytes</span><br><span class="line"># 1kb &#x3D;&gt; 1024 bytes</span><br><span class="line"># 1m &#x3D;&gt; 1000000 bytes</span><br><span class="line"># 1mb &#x3D;&gt; 1024*1024 bytes</span><br><span class="line"># 1g &#x3D;&gt; 1000000000 bytes</span><br><span class="line"># 1gb &#x3D;&gt; 1024*1024*1024 bytes</span><br><span class="line">#</span><br><span class="line"># units are case insensitive so 1GB 1Gb 1gB are all the same.</span><br><span class="line"></span><br><span class="line"># TCP listen() backlog.</span><br><span class="line">#</span><br><span class="line"># In high requests-per-second environments you need an high backlog in order</span><br><span class="line"># to avoid slow clients connections issues. Note that the Linux kernel</span><br><span class="line"># will silently truncate it to the value of &#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn so</span><br><span class="line"># make sure to raise both the value of somaxconn and tcp_max_syn_backlog</span><br><span class="line"># in order to get the desired effect.</span><br><span class="line">设置tcp的backlog, backlog是一个连接队列, backlog队列总和 &#x3D; 未完成三次握手队列 + 已完成三次握手队列</span><br></pre></td></tr></table></figure>

<h2 id="3-Redis-持久化"><a href="#3-Redis-持久化" class="headerlink" title="3. Redis 持久化"></a>3. Redis 持久化</h2><h3 id="1-RDB-redis-database"><a href="#1-RDB-redis-database" class="headerlink" title="1. RDB(redis database)"></a>1. RDB(redis database)</h3><ul>
<li><p>在指定的时间间隔内将内存中的数据集快照写入磁盘, 即Snapshot快照, 它恢复时是将快照文件直接读入到内存中</p>
</li>
<li><p>特点:</p>
<ul>
<li>Redis会单独创建(fork)一个子进程来持久化, 会先将数据写入到一个临时文件中, 等到持久化过程结束了,再用这个临时文件替换上次持久化号的文件  </li>
<li>整个过程中, 主进程是不进行任何IO操作的，　这就确保了极高的性能  </li>
<li>如果需要大规模数据的回复, 且对于数据恢复的完整性不是非常敏感, RDB方式要比AOF方式更高效</li>
<li>RDB的缺点是最后一次持久化后的数据可能丢失  </li>
</ul>
</li>
<li><p>Fork: 复制一个与当前进程一样的进程, 新进程的所有数据,数值和原进程一致, 但是是一个全新的进程, 并作为原进程的子进程, 但是性能会暂时2倍膨胀</p>
</li>
<li><p>Rdb保存的是dump.rdb, 在运行目录下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">################################ SNAPSHOTTING  ################################</span><br><span class="line">#</span><br><span class="line"># Save the DB on disk:</span><br><span class="line">#</span><br><span class="line">#   save &lt;seconds&gt; &lt;changes&gt;</span><br><span class="line">#</span><br><span class="line">#   Will save the DB if both the given number of seconds and the given</span><br><span class="line">#   number of write operations against the DB occurred.</span><br><span class="line">#</span><br><span class="line">#   In the example below the behaviour will be to save:</span><br><span class="line">#   after 900 sec (15 min) if at least 1 key changed</span><br><span class="line">#   after 300 sec (5 min) if at least 10 keys changed</span><br><span class="line">#   after 60 sec if at least 10000 keys changed</span><br><span class="line">#</span><br><span class="line">#   Note: you can disable saving completely by commenting out all &quot;save&quot; lines.</span><br><span class="line">#</span><br><span class="line">#   It is also possible to remove all the previously configured save</span><br><span class="line">#   points by adding a save directive with a single empty string argument</span><br><span class="line">#   like in the following example:</span><br><span class="line">#</span><br><span class="line">#   save &quot;&quot;</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure>

<ul>
<li>在命令行直接输入 save/bgsave,flushall(清空了, 无意义) 可以立刻保存<ul>
<li>save : 阻塞式保存</li>
<li>bgsave : 后台异步保存</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-AOF-append-only-file"><a href="#2-AOF-append-only-file" class="headerlink" title="2. AOF(append only file)"></a>2. AOF(append only file)</h3><ul>
<li><p>以日志的形式记录每个<strong>写</strong>操作,将redis执行过的所有写指令记录下来(读操作不记录),只可以追加文件, 不可以改写文件, redis启动之初回读取该文件重新构建数据, 即redis重启的时候根据日志文件的内容重新执行一遍写指令以恢复数据</p>
<ul>
<li>该功能默认是关闭的</li>
<li>生成的文件在dir下, appendonly.aof, 可以和dump.rdb共存, 但是首先加载aof文件  </li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">################### APPEND ONLY MODE ###################</span><br><span class="line">682 # By default Redis asynchronously dumps the dataset on disk. This mode is</span><br><span class="line">683 # good enough in many applications, but an issue with the Redis process or</span><br><span class="line">684 # a power outage may result into a few minutes of writes lost (depending on</span><br><span class="line">685 # the configured save points).</span><br><span class="line">686 #</span><br><span class="line">687 # The Append Only File is an alternative persistence mode that provides</span><br><span class="line">688 # much better durability. For instance using the default data fsync policy</span><br><span class="line">689 # (see later in the config file) Redis can lose just one second of writes in a</span><br><span class="line">690 # dramatic event like a server power outage, or a single write if something</span><br><span class="line">691 # wrong with the Redis process itself happens, but the operating system is</span><br><span class="line">692 # still running correctly.</span><br><span class="line">693 #</span><br><span class="line">694 # AOF and RDB persistence can be enabled at the same time without problems.</span><br><span class="line">695 # If the AOF is enabled on startup Redis will load the AOF, that is the file</span><br><span class="line">696 # with the better durability guarantees.</span><br><span class="line">697 #</span><br><span class="line">698 # Please check http:&#x2F;&#x2F;redis.io&#x2F;topics&#x2F;persistence for more information.</span><br><span class="line"></span><br><span class="line">700 appendonly no</span><br><span class="line"></span><br><span class="line">702 # The name of the append only file (default: &quot;appendonly.aof&quot;)</span><br><span class="line"></span><br><span class="line">704 appendfilename &quot;appendonly.aof&quot;</span><br><span class="line"></span><br><span class="line">706 # The fsync() call tells the Operating System to actually write data on disk</span><br><span class="line">707 # instead of waiting for more data in the output buffer. Some OS will really flush</span><br><span class="line">708 # data on disk, some other OS will just try to do it ASAP.</span><br><span class="line">709 #</span><br><span class="line">710 # Redis supports three different modes:</span><br><span class="line">711 #</span><br><span class="line">712 # no: don&#39;t fsync, just let the OS flush the data when it wants. Faster.</span><br><span class="line">713 # always: fsync after every write to the append only log. Slow, Safest.</span><br><span class="line">714 # everysec: fsync only one time every second. Compromise.</span><br><span class="line">715 #</span><br><span class="line">716 # The default is &quot;everysec&quot;, as that&#39;s usually the right compromise between</span><br><span class="line">717 # speed and data safety. It&#39;s up to you to understand if you can relax this to</span><br><span class="line">718 # &quot;no&quot; that will let the operating system flush the output buffer when</span><br><span class="line">719 # it wants, for better performances (but if you can live with the idea of</span><br><span class="line">720 # some data loss consider the default persistence mode that&#39;s snapshotting),</span><br><span class="line">721 # or on the contrary, use &quot;always&quot; that&#39;s very slow but a bit safer than</span><br><span class="line">722 # everysec.</span><br><span class="line">723 #</span><br><span class="line">724 # More details please check the following article:</span><br><span class="line">725 # http:&#x2F;&#x2F;antirez.com&#x2F;post&#x2F;redis-persistence-demystified.html</span><br><span class="line">726 #</span><br><span class="line">727 # If unsure, use &quot;everysec&quot;.</span><br><span class="line"></span><br><span class="line">729 # appendfsync always</span><br><span class="line">730 appendfsync everysec</span><br><span class="line">731 # appendfsync no</span><br><span class="line"></span><br><span class="line">733 # When the AOF fsync policy is set to always or everysec, and a background</span><br><span class="line">734 # saving process (a background save or AOF log background rewriting) is</span><br><span class="line">735 # performing a lot of I&#x2F;O against the disk, in some Linux configurations</span><br><span class="line">745 # the same as &quot;appendfsync none&quot;. In practical terms, this means that it is</span><br><span class="line">746 # possible to lose up to 30 seconds of log in the worst scenario (with the</span><br><span class="line">747 # default Linux settings).</span><br><span class="line">748 #</span><br><span class="line">749 # If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as</span><br><span class="line">750 # &quot;no&quot; that is the safest pick from the point of view of durability.</span><br><span class="line"></span><br><span class="line">752 no-appendfsync-on-rewrite no</span><br><span class="line"></span><br><span class="line">754 # Automatic rewrite of the append only file.</span><br><span class="line">755 # Redis is able to automatically rewrite the log file implicitly calling</span><br><span class="line">756 # BGREWRITEAOF when the AOF log size grows by the specified percentage.</span><br><span class="line">757 #</span><br><span class="line">758 # This is how it works: Redis remembers the size of the AOF file after the</span><br><span class="line">759 # latest rewrite (if no rewrite has happened since the restart, the size of</span><br><span class="line">760 # the AOF at startup is used).</span><br><span class="line">761 #</span><br><span class="line">762 # This base size is compared to the current size. If the current size is</span><br><span class="line">763 # bigger than the specified percentage, the rewrite is triggered. Also</span><br><span class="line">764 # you need to specify a minimal size for the AOF file to be rewritten, this</span><br><span class="line">765 # is useful to avoid rewriting the AOF file even if the percentage increase</span><br><span class="line">766 # is reached but it is still pretty small.</span><br><span class="line">767 #</span><br><span class="line">768 # Specify a percentage of zero in order to disable the automatic AOF</span><br><span class="line">769 # rewrite feature.</span><br><span class="line"></span><br><span class="line">771 auto-aof-rewrite-percentage 100</span><br><span class="line">772 auto-aof-rewrite-min-size 64mb</span><br><span class="line"></span><br><span class="line">774 # An AOF file may be found to be truncated at the end during the Redis</span><br><span class="line">775 # startup process, when the AOF data gets loaded back into memory.</span><br><span class="line">776 # This may happen when the system where Redis is running</span><br><span class="line">777 # crashes, especially when an ext4 filesystem is mounted without the</span><br><span class="line">778 # data&#x3D;ordered option (however this can&#39;t happen when Redis itself</span><br><span class="line">779 # crashes or aborts but the operating system still works correctly).</span><br><span class="line">780 #</span><br><span class="line">781 # Redis can either exit with an error when this happens, or load as much</span><br><span class="line">782 # data as possible (the default now) and start if the AOF file is found</span><br><span class="line">783 # to be truncated at the end. The following option controls this behavior.</span><br><span class="line">784 #</span><br><span class="line">785 # If aof-load-truncated is set to yes, a truncated AOF file is loaded and</span><br><span class="line">786 # the Redis server starts emitting a log to inform the user of the event.</span><br><span class="line">787 # Otherwise if the option is set to no, the server aborts with an error</span><br><span class="line">788 # and refuses to start. When the option is set to no, the user requires</span><br><span class="line">789 # to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart</span><br><span class="line">790 # the server.</span><br><span class="line">791 #</span><br><span class="line">792 # Note that if the AOF file will be found to be corrupted in the middle</span><br><span class="line">793 # the server will still exit with an error. This option only applies when</span><br><span class="line">794 # Redis will try to read more data from the AOF file but not enough bytes</span><br><span class="line">795 # will be found.</span><br><span class="line">796 aof-load-truncated yes</span><br><span class="line"></span><br><span class="line">798 # When rewriting the AOF file, Redis is able to use an RDB preamble in the</span><br><span class="line">799 # AOF file for faster rewrites and recoveries. When this option is turned</span><br><span class="line">800 # on the rewritten AOF file is composed of two different stanzas:</span><br><span class="line">801 #</span><br><span class="line">802 #   [RDB file][AOF tail]</span><br><span class="line">803 #</span><br><span class="line">804 # When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;</span><br><span class="line">805 # string and loads the prefixed RDB file, and continues loading the AOF</span><br><span class="line">806 # tail.</span><br><span class="line">807 aof-use-rdb-preamble yes</span><br><span class="line"></span><br><span class="line">809 ################## LUA SCRIPTING  ################</span><br></pre></td></tr></table></figure>
</li>
<li><p>总结</p>
<ul>
<li>AOF文件是一个只进行追加的日志文件</li>
<li>redis可以在AOF文件体积变得过大时, 自动地在后台对AOF进行重写</li>
<li>AOF文件有序地保存了对数据库执行的所有写入操作, 这些写入操作以reids协作的格式保存, 因此,AIF文件的内容容易读懂</li>
<li>对相同的数据集来说, AOF文件的体积通常大于RDB文件</li>
<li>根据所使用的fsync策略, AOF的速度可能会慢于RDB</li>
</ul>
</li>
</ul>
<h3 id="3-小总结"><a href="#3-小总结" class="headerlink" title="3. 小总结"></a>3. 小总结</h3><ul>
<li><p>RDB持久化方式能在指定的时间间隔内对数据进行快照存储</p>
</li>
<li><p>AOF持久化方式记录每次对服务器写的操作, 当服务器重启的时候, 会重新执行这些命令来恢复原始的数据, redis可以对AOF文件后台重写, 使得AOF文件的体积不至于过大</p>
</li>
<li><p>只做缓存: 即数据值在服务器运行的时候存在, 可以不用持久化</p>
</li>
<li><p>作者建议同时开启2种持久化方式:  </p>
<ul>
<li>优先载入AOF文件, 因为通常AOF要比RDB存储的完整</li>
<li>RDB更适合于备份数据库(AOF不断变化, 不适合备份), 留一手</li>
</ul>
<p><img src="./img/rob&aof.png" alt="RDB&amp;AOP性能建议"></p>
</li>
</ul>
<h2 id="4-Redis-事务"><a href="#4-Redis-事务" class="headerlink" title="4. Redis 事务"></a>4. Redis 事务</h2><ul>
<li><p>概念:  </p>
<ul>
<li>可以一次执行多个命令, 本质是<strong>一组命令的集合</strong>, 一个事务种的所有命令都会序列化, 按顺序的串行化执行,而不会被其他命令插入</li>
<li>一个队列中, 一次性, 顺序性,排他性的执行一系列命令</li>
</ul>
</li>
<li><p>常用命令:</p>
<ol>
<li>discard: 取消事务, 放弃执行事务块内的所有命令</li>
<li>exec: 执行事务块内的所有命令</li>
<li>multi: 标志着事务块的开启</li>
<li>unwatch: 取消watch命令对所有key的监视</li>
<li>watch key: 监视1个或多个 key, 如果在事务执行前,这个key被其他命令所改动, 那么事务将被打断</li>
</ol>
</li>
<li><p>redis 部分支持事务</p>
<ul>
<li>命令全部正确, 正常批量执行</li>
<li>discard 放弃执行所有事务</li>
<li>全体连坐 — 有严重错误命令, 直接报错, 全部失败</li>
<li>冤头寨主 — 小的错误, 比如给字母 incr 1, 只有该条命令失败</li>
<li>watch监控<ul>
<li>使用的是乐观锁</li>
<li>悲观锁/乐观锁/CAS(check and set)<ol>
<li>悲观锁:(认为一定会出事, 在MySQL中就是把整张表都锁了,每次拿数据的时候都会上锁, 并发差, 一致性好)</li>
<li>乐观锁:(认为不会出事, 不会上锁,在一张表后加一个version字段; 一旦有并发的修改, 在提交的时候,如果版本号和查询的时候不相同, 就失败, 先把数据更新到别人改好的版本, 再进行自己的修改, 直到修改成功): 提交版本必须大于当前记录版本才更新</li>
</ol>
</li>
<li>一旦执行了exec, 之前的监控锁都被取消</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="插曲-Redis-发布和订阅"><a href="#插曲-Redis-发布和订阅" class="headerlink" title="插曲: Redis 发布和订阅"></a>插曲: Redis 发布和订阅</h3><ul>
<li>进程间的一种消息通信模式: 发送者(pub)发送消息, 订阅者(sub)接收消息</li>
</ul>
<h2 id="5-Redis的复制-master-slave"><a href="#5-Redis的复制-master-slave" class="headerlink" title="5. Redis的复制(master/slave)"></a>5. Redis的复制(master/slave)</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1. 概念"></a>1. 概念</h3><ul>
<li>主从复制, 主机数据更新后根据配置和策略, 自动同步到备机的master/slave机制, master以写为主, slave以读为主</li>
<li>读写分离, 容灾恢复  </li>
</ul>
<h3 id="2-怎么用"><a href="#2-怎么用" class="headerlink" title="2. 怎么用"></a>2. 怎么用</h3><ol>
<li>配从(库)不配主(库)</li>
<li>从库配置: slaveof 主库IP 主库端口<ul>
<li>每次从机与master断开之后, 都要重新连接, 除非配置到redis.conf文件中</li>
<li>Info replication</li>
</ul>
</li>
<li>修改配置文件细节:<ul>
<li>拷贝多个redis.conf文件</li>
<li>开启daemonize yes</li>
<li>pid文件名字</li>
<li>指定端口</li>
<li>log文件名字</li>
<li>dump.rdb名字</li>
</ul>
</li>
<li>常用3招:<ol>
<li>一主二仆<ul>
<li>如果主机主动断开, 从机默认不发生变化, 主机重新连接后, 依然保持不变, 但是从机主动断开后需要重新连接主机</li>
</ul>
</li>
<li>薪火相传</li>
<li>反客为主: slaceof no one 使当前数据库停止与其他数据库的同步,转为u主数据库</li>
</ol>
</li>
</ol>
<h3 id="3-复制原理"><a href="#3-复制原理" class="headerlink" title="3.  复制原理"></a>3.  复制原理</h3><ul>
<li>slave启动成功连接到master后会发送一个sync命令</li>
<li>master接到命令后, 启动后台的存盘程序, 同时收集所有接收到的用于修改数据集命令, 在后台进程执行完毕后, master将传送整个数据文件到slave, 已完成一次完全同步</li>
<li>全量复制: 而slave服务在接受到数据库文件数据后, 将其存盘并加载到内存中</li>
<li>增量复制: master继续将新的所有收集到的修改命令一次传给salve, 完成同步</li>
<li>但是只要是重新连接master, 一次完全同步将被自动执行</li>
</ul>
<h3 id="4-哨兵模式-sentinel"><a href="#4-哨兵模式-sentinel" class="headerlink" title="4. 哨兵模式(sentinel)"></a>4. 哨兵模式(sentinel)</h3><ul>
<li><p>反客为主的自动版, 后台监控主机是否故障,如果故障, 根据投票数将从库转换为主库</p>
</li>
<li><p>步骤:</p>
<ul>
<li>新建sentinel.conf文件跟之前的配置文件在同一目录</li>
<li>配置哨兵, 填写内容<ul>
<li>sentinel monitor 被监控数据库名字 ip:端口号 1<br>数字1表示主机挂掉后,slave投票看让谁接替成为宿主机, 票数多的成为主机</li>
</ul>
</li>
<li>启动哨兵<ul>
<li>redis-sentinel …./sentinel.conf</li>
</ul>
</li>
<li>挂掉的主机重启回来后, 称为新的主机的slave</li>
</ul>
</li>
<li><p>复制的缺点</p>
<ul>
<li>延迟</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>jdk8中HashMap更新映射项的方法(merge)笔记</title>
    <url>/posts/85124b9.html</url>
    <content><![CDATA[<ul>
<li><p>在jdk1.8中, HashMap中更新了几个关于更新映射项的方法, 个人觉得很好用,于是写个笔记小记一下.</p>
<ul>
<li><code>public V getOrDefault(Object key, V defaultValue)</code></li>
<li><code>public V putIfAbsent(K key, V value)</code>  </li>
<li><code>public V merge(K key, V value, BiFunction&lt;? super V,? super V,? extends V&gt; remappingFunction)</code></li>
</ul>
</li>
<li><p>处理映射项的一个难点是更新映射项, 也就是我们常说的键值对中的值. 正常情况下, 可以通过<code>get(key)</code>方法获得值, 然后对其进行更新, 然后再放回更新的值. 但是要考虑到键第一次出现的时候. 比如, 再统计某个单词在文件中出现的频度.  </p>
</li>
<li><p>在看到一个单词(word)时, 将对应的计数器加1, 如下所示</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Map&lt;String, Integer&gt; counts = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">String word = <span class="string">"word"</span>;</span><br><span class="line">counts.put(word, counts.get(word)+<span class="number">1</span>); <span class="comment">// NullPointerException, 第一次获取"1"时get返回null</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在jdk8之前我们解决这个问题, 应该就会在put前进行检测, 虽然不难, 但是当jdk8后可以更加便捷的方法.  </p>
<ul>
<li><p>第一种方法就是利用<code>getOrDefault</code>方法:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">counts.put(word, counts.getOrDefault(word, <span class="number">0</span>) + <span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>api中的描述是:<blockquote>
<p>Returns the value to which the specified key is mapped, or if this map contains no mapping for the key.</p>
</blockquote>
</li>
<li>直接看源码:</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">default</span> V <span class="title">getOrDefault</span><span class="params">(Object key, V defaultValue)</span> </span>&#123;</span><br><span class="line">    V v;</span><br><span class="line">    <span class="keyword">return</span> (((v = get(key)) != <span class="keyword">null</span>) || containsKey(key)) ? v : defaultValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>发现这个默认方法其实就是把我们之前做的事简单封装了一下, 简单明了</p>
</li>
<li><p>第二种方法是先调用putIfAbsent方法, 只有当键存在时才放入一个值, 这跟我们之前的做法是一样的,只不过更加便捷.  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">counts.putIfAbsent(word,<span class="number">0</span>); <span class="comment">// 直接看源代码, 清楚明白</span></span><br><span class="line">counts.put(word, counts.get(word)+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">default</span> V <span class="title">putIfAbsent</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">    V v = get(key);</span><br><span class="line">    <span class="keyword">if</span> (v == <span class="keyword">null</span>) &#123;</span><br><span class="line">        v = put(key, value);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> v;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>看到源码, 我不知道怎么形容了, 就是2个字, 清晰</li>
<li>第三种方法就是使用merge方法, 这也是最方便的方法, 强烈建议:</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">counts.merge(word, <span class="number">1</span>, Integer::sum);  </span><br><span class="line"></span><br><span class="line"><span class="comment">// 用方法引用传入sum方法, 当word第一次出现时, word的值被赋值为1,  </span></span><br><span class="line"><span class="comment">// 不是第一次出现时, 会将其值加1.</span></span><br><span class="line"><span class="comment">// 这是一个函数式接口, 用来写怎样处理默认值value和get方法得到的oldValue之间的关系</span></span><br><span class="line"><span class="function"><span class="keyword">default</span> V <span class="title">merge</span><span class="params">(K key, V value,  BiFunction&lt;? <span class="keyword">super</span> V, ? <span class="keyword">super</span> V, ? extends V&gt; remappingFunction)</span>   </span>&#123;</span><br><span class="line">    <span class="comment">// 确保remappingFunction 不是一个 null 值,  </span></span><br><span class="line">    <span class="comment">// 如果是null会直接抛异常, 不是null就返回对象本身.</span></span><br><span class="line">    Objects.requireNonNull(remappingFunction);  </span><br><span class="line">    Objects.requireNonNull(value);  </span><br><span class="line">    <span class="comment">// 获取当前key中的值, 如果是第一次出现的键, 会得到一个null</span></span><br><span class="line">    V oldValue = get(key);  </span><br><span class="line">    V newValue = (oldValue == <span class="keyword">null</span>) ? value :</span><br><span class="line">                remappingFunction.apply(oldValue, value);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    当oldVlue为null时, 也就是说该键key第一次出现, 返回默认值value,</span></span><br><span class="line"><span class="comment">    不是第一次出现的时候, 交给函数式接口来处理, 在调用的时候可以用lambda表达式或方法引用来写</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">if</span>(newValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">        remove(key); <span class="comment">// 如果经过以上处理, 得到的newValue还是null的话, 删去该key, 否则就正常的添加该键值对,  </span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        put(key, newValue);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 方法最终返回我们想要的值</span></span><br><span class="line">    <span class="keyword">return</span> newValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>都在注释里写清楚了.</li>
<li>综上, 以后再更新映射项就考虑merge吧!</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>参考文献</strong>:<br>Java核心技术 卷1 原书第10版 , 第9章</p>
</blockquote>
]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
</search>
